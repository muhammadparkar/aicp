{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLnnTw9a2SQ1",
    "outputId": "ff091e06-5fb1-45b6-86fb-5d56fca9513b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in /home/talisman/.local/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/talisman/.local/lib/python3.12/site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: colorlog in /home/talisman/.local/lib/python3.12/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in /home/talisman/.local/lib/python3.12/site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/talisman/.local/lib/python3.12/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/talisman/.local/lib/python3.12/site-packages (from optuna) (2.0.35)\n",
      "Requirement already satisfied: tqdm in /home/talisman/.local/lib/python3.12/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /home/talisman/.local/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /home/talisman/.local/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/talisman/.local/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/talisman/.local/lib/python3.12/site-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/talisman/.local/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giz9L6o53KWZ",
    "outputId": "0d188563-ce9e-42d3-d3fb-ab818e437961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in /home/talisman/.local/lib/python3.12/site-packages (2.1.1)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/talisman/.local/lib/python3.12/site-packages (from xgboost) (2.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/talisman/.local/lib/python3.12/site-packages (from xgboost) (2.22.3)\n",
      "Requirement already satisfied: scipy in /home/talisman/.local/lib/python3.12/site-packages (from xgboost) (1.14.1)\n",
      "Downloading xgboost-2.1.2-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 2.1.1\n",
      "    Uninstalling xgboost-2.1.1:\n",
      "      Successfully uninstalled xgboost-2.1.1\n",
      "Successfully installed xgboost-2.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5c8VgsxszwB",
    "outputId": "12800312-25c7-462f-b8e4-9fde5d4aff7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/talisman/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-11-10 11:47:52,770] A new study created in memory with name: no-name-a1d99f7c-96d9-4b30-8010-00ae0d3e87d9\n",
      "[I 2024-11-10 11:47:53,555] Trial 0 finished with value: 0.8135449711971112 and parameters: {'model': 'Gradient Boosting', 'learning_rate': 0.03989183927265595, 'n_estimators': 489, 'max_depth': 5, 'min_samples_split': 3}. Best is trial 0 with value: 0.8135449711971112.\n",
      "[I 2024-11-10 11:47:53,939] Trial 1 finished with value: 0.7700737115190077 and parameters: {'model': 'Gradient Boosting', 'learning_rate': 0.18754175947476098, 'n_estimators': 155, 'max_depth': 10, 'min_samples_split': 6}. Best is trial 0 with value: 0.8135449711971112.\n",
      "[I 2024-11-10 11:47:54,790] Trial 2 finished with value: 0.8350210785865784 and parameters: {'model': 'XGBoost', 'learning_rate': 0.12567009801344567, 'max_depth': 3, 'n_estimators': 438, 'min_child_weight': 10, 'subsample': 0.8644201308611823, 'gamma': 1.5614815466097431}. Best is trial 2 with value: 0.8350210785865784.\n",
      "[I 2024-11-10 11:47:55,186] Trial 3 finished with value: 0.842049777507782 and parameters: {'model': 'XGBoost', 'learning_rate': 0.1462868103182176, 'max_depth': 9, 'n_estimators': 162, 'min_child_weight': 3, 'subsample': 0.6960957559090832, 'gamma': 0.3932885852401269}. Best is trial 3 with value: 0.842049777507782.\n",
      "[I 2024-11-10 11:47:56,103] Trial 4 finished with value: 0.8774173327101317 and parameters: {'model': 'Random Forest', 'n_estimators': 422, 'max_depth': 15}. Best is trial 4 with value: 0.8774173327101317.\n",
      "[I 2024-11-10 11:47:56,143] Trial 5 finished with value: -0.03490568284082851 and parameters: {'model': 'SVM', 'svm_kernel': 'poly', 'svm_C': 3.9843699030589774}. Best is trial 4 with value: 0.8774173327101317.\n",
      "[I 2024-11-10 11:47:56,588] Trial 6 finished with value: 0.836084246635437 and parameters: {'model': 'XGBoost', 'learning_rate': 0.18629866569996417, 'max_depth': 8, 'n_estimators': 206, 'min_child_weight': 8, 'subsample': 0.6340440861273648, 'gamma': 3.9972658754696004}. Best is trial 4 with value: 0.8774173327101317.\n",
      "[I 2024-11-10 11:47:56,611] Trial 7 finished with value: 0.7133944270278739 and parameters: {'model': 'Linear Regression'}. Best is trial 4 with value: 0.8774173327101317.\n",
      "[I 2024-11-10 11:47:56,981] Trial 8 finished with value: 0.849359393119812 and parameters: {'model': 'XGBoost', 'learning_rate': 0.15157498201400466, 'max_depth': 5, 'n_estimators': 186, 'min_child_weight': 3, 'subsample': 0.9093183184790485, 'gamma': 0.7758322059577882}. Best is trial 4 with value: 0.8774173327101317.\n",
      "[I 2024-11-10 11:47:57,027] Trial 9 finished with value: -0.0396582104872929 and parameters: {'model': 'SVM', 'svm_kernel': 'rbf', 'svm_C': 9.951686495177066}. Best is trial 4 with value: 0.8774173327101317.\n",
      "[I 2024-11-10 11:47:57,795] Trial 10 finished with value: 0.8815207482111092 and parameters: {'model': 'Random Forest', 'n_estimators': 359, 'max_depth': 18}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:47:58,654] Trial 11 finished with value: 0.8780928201127 and parameters: {'model': 'Random Forest', 'n_estimators': 366, 'max_depth': 18}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:47:59,515] Trial 12 finished with value: 0.8801603463714234 and parameters: {'model': 'Random Forest', 'n_estimators': 334, 'max_depth': 20}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:00,145] Trial 13 finished with value: 0.8775366836938925 and parameters: {'model': 'Random Forest', 'n_estimators': 291, 'max_depth': 20}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:00,837] Trial 14 finished with value: 0.8790852792651658 and parameters: {'model': 'Random Forest', 'n_estimators': 308, 'max_depth': 16}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:01,460] Trial 15 finished with value: 0.880541630146773 and parameters: {'model': 'Random Forest', 'n_estimators': 290, 'max_depth': 14}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:01,467] Trial 16 finished with value: 0.7133944270278739 and parameters: {'model': 'Linear Regression'}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:01,955] Trial 17 finished with value: 0.8768410753131687 and parameters: {'model': 'Random Forest', 'n_estimators': 228, 'max_depth': 13}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:02,606] Trial 18 finished with value: 0.8765186668141123 and parameters: {'model': 'Random Forest', 'n_estimators': 254, 'max_depth': 13}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:03,475] Trial 19 finished with value: 0.8811169373501154 and parameters: {'model': 'Random Forest', 'n_estimators': 386, 'max_depth': 16}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:03,481] Trial 20 finished with value: 0.7133944270278739 and parameters: {'model': 'Linear Regression'}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:04,346] Trial 21 finished with value: 0.8785745145599827 and parameters: {'model': 'Random Forest', 'n_estimators': 377, 'max_depth': 17}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:05,190] Trial 22 finished with value: 0.8717363434861631 and parameters: {'model': 'Random Forest', 'n_estimators': 377, 'max_depth': 14}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:06,057] Trial 23 finished with value: 0.8773670517047726 and parameters: {'model': 'Random Forest', 'n_estimators': 418, 'max_depth': 18}. Best is trial 10 with value: 0.8815207482111092.\n",
      "[I 2024-11-10 11:48:06,654] Trial 24 finished with value: 0.8828201218161087 and parameters: {'model': 'Random Forest', 'n_estimators': 288, 'max_depth': 12}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:06,812] Trial 25 finished with value: 0.7983384355049948 and parameters: {'model': 'Gradient Boosting', 'learning_rate': 0.27310921461311477, 'n_estimators': 71, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:06,852] Trial 26 finished with value: 0.5134623866306545 and parameters: {'model': 'SVM', 'svm_kernel': 'linear', 'svm_C': 0.7798387659715171}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:07,589] Trial 27 finished with value: 0.877969027814552 and parameters: {'model': 'Random Forest', 'n_estimators': 340, 'max_depth': 12}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:08,529] Trial 28 finished with value: 0.8781646603637542 and parameters: {'model': 'Random Forest', 'n_estimators': 454, 'max_depth': 16}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:09,008] Trial 29 finished with value: 0.8206867021663463 and parameters: {'model': 'Gradient Boosting', 'learning_rate': 0.01692472179554949, 'n_estimators': 251, 'max_depth': 7, 'min_samples_split': 10}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:09,859] Trial 30 finished with value: 0.8778384308323275 and parameters: {'model': 'Random Forest', 'n_estimators': 393, 'max_depth': 11}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:10,555] Trial 31 finished with value: 0.8808110778375414 and parameters: {'model': 'Random Forest', 'n_estimators': 297, 'max_depth': 15}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:11,247] Trial 32 finished with value: 0.878648316680555 and parameters: {'model': 'Random Forest', 'n_estimators': 332, 'max_depth': 18}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:11,947] Trial 33 finished with value: 0.68382230981585 and parameters: {'model': 'Gradient Boosting', 'learning_rate': 0.2894166398118092, 'n_estimators': 327, 'max_depth': 9, 'min_samples_split': 2}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:12,551] Trial 34 finished with value: 0.8795336348474344 and parameters: {'model': 'Random Forest', 'n_estimators': 276, 'max_depth': 16}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:13,362] Trial 35 finished with value: 0.8815651244800439 and parameters: {'model': 'Random Forest', 'n_estimators': 361, 'max_depth': 19}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:13,405] Trial 36 finished with value: -0.03973292394813255 and parameters: {'model': 'SVM', 'svm_kernel': 'rbf', 'svm_C': 9.91561436434285}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:14,842] Trial 37 finished with value: 0.8549312353134155 and parameters: {'model': 'XGBoost', 'learning_rate': 0.08964427446359118, 'max_depth': 7, 'n_estimators': 489, 'min_child_weight': 1, 'subsample': 0.50323398757423, 'gamma': 4.837104888978443}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:14,859] Trial 38 finished with value: 0.7133944270278739 and parameters: {'model': 'Linear Regression'}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:15,628] Trial 39 finished with value: 0.8800188358765945 and parameters: {'model': 'Random Forest', 'n_estimators': 354, 'max_depth': 19}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:16,175] Trial 40 finished with value: 0.808140754699707 and parameters: {'model': 'XGBoost', 'learning_rate': 0.23454576090240908, 'max_depth': 5, 'n_estimators': 406, 'min_child_weight': 6, 'subsample': 0.9992533960791555, 'gamma': 2.897494034309087}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:16,879] Trial 41 finished with value: 0.8810217780101677 and parameters: {'model': 'Random Forest', 'n_estimators': 313, 'max_depth': 17}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:17,525] Trial 42 finished with value: 0.8799117019313093 and parameters: {'model': 'Random Forest', 'n_estimators': 317, 'max_depth': 19}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:18,279] Trial 43 finished with value: 0.877321362174759 and parameters: {'model': 'Random Forest', 'n_estimators': 349, 'max_depth': 17}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:19,156] Trial 44 finished with value: 0.8732587712967852 and parameters: {'model': 'Random Forest', 'n_estimators': 462, 'max_depth': 19}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:19,916] Trial 45 finished with value: 0.8756048837498692 and parameters: {'model': 'Random Forest', 'n_estimators': 392, 'max_depth': 17}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:19,955] Trial 46 finished with value: 0.5301735686864513 and parameters: {'model': 'SVM', 'svm_kernel': 'linear', 'svm_C': 5.760845712845843}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:20,497] Trial 47 finished with value: 0.8796954894855761 and parameters: {'model': 'Random Forest', 'n_estimators': 269, 'max_depth': 15}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:21,159] Trial 48 finished with value: 0.7721248517333188 and parameters: {'model': 'Gradient Boosting', 'learning_rate': 0.07757624046503513, 'n_estimators': 364, 'max_depth': 8, 'min_samples_split': 6}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:21,531] Trial 49 finished with value: 0.8142402172088623 and parameters: {'model': 'XGBoost', 'learning_rate': 0.23670639042190966, 'max_depth': 3, 'n_estimators': 437, 'min_child_weight': 10, 'subsample': 0.5117304632749909, 'gamma': 2.645161355345624}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:22,183] Trial 50 finished with value: 0.8801406584522014 and parameters: {'model': 'Random Forest', 'n_estimators': 315, 'max_depth': 20}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:22,834] Trial 51 finished with value: 0.8785283243719305 and parameters: {'model': 'Random Forest', 'n_estimators': 298, 'max_depth': 15}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:23,561] Trial 52 finished with value: 0.8765232681538958 and parameters: {'model': 'Random Forest', 'n_estimators': 350, 'max_depth': 18}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:24,179] Trial 53 finished with value: 0.8774357713650942 and parameters: {'model': 'Random Forest', 'n_estimators': 288, 'max_depth': 15}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:24,189] Trial 54 finished with value: 0.7133944270278739 and parameters: {'model': 'Linear Regression'}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:24,832] Trial 55 finished with value: 0.8791505795367524 and parameters: {'model': 'Random Forest', 'n_estimators': 261, 'max_depth': 17}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:25,381] Trial 56 finished with value: 0.8768573983484726 and parameters: {'model': 'Random Forest', 'n_estimators': 231, 'max_depth': 11}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:26,052] Trial 57 finished with value: 0.8760423886903329 and parameters: {'model': 'Random Forest', 'n_estimators': 306, 'max_depth': 14}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:26,897] Trial 58 finished with value: 0.8793542366199315 and parameters: {'model': 'Random Forest', 'n_estimators': 372, 'max_depth': 16}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:27,730] Trial 59 finished with value: 0.8718432603720571 and parameters: {'model': 'Random Forest', 'n_estimators': 327, 'max_depth': 13}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:27,785] Trial 60 finished with value: -0.023254897720346168 and parameters: {'model': 'SVM', 'svm_kernel': 'poly', 'svm_C': 5.83613875062809}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:28,544] Trial 61 finished with value: 0.8774878492603159 and parameters: {'model': 'Random Forest', 'n_estimators': 279, 'max_depth': 14}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:29,257] Trial 62 finished with value: 0.8776290903216778 and parameters: {'model': 'Random Forest', 'n_estimators': 296, 'max_depth': 14}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:29,877] Trial 63 finished with value: 0.8770300887127722 and parameters: {'model': 'Random Forest', 'n_estimators': 231, 'max_depth': 12}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:30,646] Trial 64 finished with value: 0.8813774219546546 and parameters: {'model': 'Random Forest', 'n_estimators': 384, 'max_depth': 17}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:31,411] Trial 65 finished with value: 0.8787330477750969 and parameters: {'model': 'Random Forest', 'n_estimators': 399, 'max_depth': 19}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:31,419] Trial 66 finished with value: 0.7133944270278739 and parameters: {'model': 'Linear Regression'}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:32,151] Trial 67 finished with value: 0.878565486867865 and parameters: {'model': 'Random Forest', 'n_estimators': 386, 'max_depth': 17}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:32,997] Trial 68 finished with value: 0.7765047215557531 and parameters: {'model': 'Gradient Boosting', 'learning_rate': 0.09517969546453867, 'n_estimators': 426, 'max_depth': 9, 'min_samples_split': 8}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:33,699] Trial 69 finished with value: 0.881105563481229 and parameters: {'model': 'Random Forest', 'n_estimators': 363, 'max_depth': 18}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:34,391] Trial 70 finished with value: 0.8818205456105978 and parameters: {'model': 'Random Forest', 'n_estimators': 361, 'max_depth': 18}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:35,098] Trial 71 finished with value: 0.8791769147161036 and parameters: {'model': 'Random Forest', 'n_estimators': 360, 'max_depth': 18}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:35,875] Trial 72 finished with value: 0.879480289319826 and parameters: {'model': 'Random Forest', 'n_estimators': 411, 'max_depth': 20}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:36,550] Trial 73 finished with value: 0.8779638478394438 and parameters: {'model': 'Random Forest', 'n_estimators': 344, 'max_depth': 18}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:37,298] Trial 74 finished with value: 0.8776162615723713 and parameters: {'model': 'Random Forest', 'n_estimators': 383, 'max_depth': 19}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:37,762] Trial 75 finished with value: 0.8149142861366272 and parameters: {'model': 'XGBoost', 'learning_rate': 0.2015971260587009, 'max_depth': 5, 'n_estimators': 370, 'min_child_weight': 6, 'subsample': 0.7965173013391337, 'gamma': 3.7345957142963115}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:38,508] Trial 76 finished with value: 0.876210412998232 and parameters: {'model': 'Random Forest', 'n_estimators': 366, 'max_depth': 16}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:39,156] Trial 77 finished with value: 0.8762506770870703 and parameters: {'model': 'Random Forest', 'n_estimators': 327, 'max_depth': 18}. Best is trial 24 with value: 0.8828201218161087.\n",
      "[I 2024-11-10 11:48:39,830] Trial 78 finished with value: 0.8831756136168133 and parameters: {'model': 'Random Forest', 'n_estimators': 345, 'max_depth': 17}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:39,861] Trial 79 finished with value: 0.4829541720831889 and parameters: {'model': 'SVM', 'svm_kernel': 'linear', 'svm_C': 0.14756263638448708}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:40,498] Trial 80 finished with value: 0.8810881669920909 and parameters: {'model': 'Random Forest', 'n_estimators': 337, 'max_depth': 19}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:41,173] Trial 81 finished with value: 0.8820842248307835 and parameters: {'model': 'Random Forest', 'n_estimators': 355, 'max_depth': 19}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:41,947] Trial 82 finished with value: 0.8742095841624196 and parameters: {'model': 'Random Forest', 'n_estimators': 383, 'max_depth': 20}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:42,620] Trial 83 finished with value: 0.8779227266977436 and parameters: {'model': 'Random Forest', 'n_estimators': 357, 'max_depth': 18}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:43,384] Trial 84 finished with value: 0.8791287884131692 and parameters: {'model': 'Random Forest', 'n_estimators': 404, 'max_depth': 19}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:44,015] Trial 85 finished with value: 0.8790692182516415 and parameters: {'model': 'Random Forest', 'n_estimators': 342, 'max_depth': 17}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:44,269] Trial 86 finished with value: 0.7637267201440927 and parameters: {'model': 'Gradient Boosting', 'learning_rate': 0.050717304450906484, 'n_estimators': 151, 'max_depth': 7, 'min_samples_split': 4}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:44,274] Trial 87 finished with value: 0.7133944270278739 and parameters: {'model': 'Linear Regression'}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:44,986] Trial 88 finished with value: 0.8787055543313996 and parameters: {'model': 'Random Forest', 'n_estimators': 394, 'max_depth': 20}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:45,322] Trial 89 finished with value: 0.8161133527755737 and parameters: {'model': 'XGBoost', 'learning_rate': 0.2428916908712131, 'max_depth': 8, 'n_estimators': 422, 'min_child_weight': 1, 'subsample': 0.6078716032234168, 'gamma': 1.681139815624166}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:46,010] Trial 90 finished with value: 0.8791630661214617 and parameters: {'model': 'Random Forest', 'n_estimators': 376, 'max_depth': 18}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:46,623] Trial 91 finished with value: 0.8787150071662126 and parameters: {'model': 'Random Forest', 'n_estimators': 335, 'max_depth': 19}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:47,322] Trial 92 finished with value: 0.8784086407372057 and parameters: {'model': 'Random Forest', 'n_estimators': 353, 'max_depth': 19}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:48,099] Trial 93 finished with value: 0.8802092201511549 and parameters: {'model': 'Random Forest', 'n_estimators': 361, 'max_depth': 17}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:48,691] Trial 94 finished with value: 0.8826876947113882 and parameters: {'model': 'Random Forest', 'n_estimators': 322, 'max_depth': 18}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:49,376] Trial 95 finished with value: 0.873671193649468 and parameters: {'model': 'Random Forest', 'n_estimators': 321, 'max_depth': 18}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:50,196] Trial 96 finished with value: 0.8782363115024473 and parameters: {'model': 'Random Forest', 'n_estimators': 347, 'max_depth': 16}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:50,937] Trial 97 finished with value: 0.8787819546484857 and parameters: {'model': 'Random Forest', 'n_estimators': 309, 'max_depth': 17}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:51,848] Trial 98 finished with value: 0.8795525561935402 and parameters: {'model': 'Random Forest', 'n_estimators': 376, 'max_depth': 18}. Best is trial 78 with value: 0.8831756136168133.\n",
      "[I 2024-11-10 11:48:51,890] Trial 99 finished with value: -0.05330637606836941 and parameters: {'model': 'SVM', 'svm_kernel': 'rbf', 'svm_C': 3.4085630505526856}. Best is trial 78 with value: 0.8831756136168133.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics-----\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Random Forest MSE: 5119930.0291\n",
      "Random Forest RMSE: 2262.7262\n",
      "Random Forest R-squared: 0.8799\n",
      "Random Forest MAPE: 4.47%\n",
      "\n",
      "Evaluating SVM...\n",
      "SVM MSE: 20710921.6290\n",
      "SVM RMSE: 4550.9254\n",
      "SVM R-squared: 0.5143\n",
      "SVM MAPE: 12.04%\n",
      "\n",
      "Evaluating XGBoost...\n",
      "XGBoost MSE: 8478340.8757\n",
      "XGBoost RMSE: 2911.7591\n",
      "XGBoost R-squared: 0.8012\n",
      "XGBoost MAPE: 4.72%\n",
      "\n",
      "Evaluating Linear Regression...\n",
      "Linear Regression MSE: 12221661.7059\n",
      "Linear Regression RMSE: 3495.9493\n",
      "Linear Regression R-squared: 0.7134\n",
      "Linear Regression MAPE: 10.96%\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "Gradient Boosting MSE: 14074829.2345\n",
      "Gradient Boosting RMSE: 3751.6435\n",
      "Gradient Boosting R-squared: 0.6699\n",
      "Gradient Boosting MAPE: 5.22%\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Medicalpremium.csv')\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop('PremiumPrice', axis=1)  # Features (all columns except target)\n",
    "y = data['PremiumPrice']                # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical('model', ['Random Forest', 'XGBoost', 'Linear Regression', 'SVM', 'Gradient Boosting'])\n",
    "\n",
    "    if model_name == 'Random Forest':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20)\n",
    "        }\n",
    "        model = RandomForestRegressor(**params)\n",
    "\n",
    "    elif model_name == 'XGBoost':\n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "\n",
    "    elif model_name == 'SVM':\n",
    "        params = {\n",
    "            'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'rbf', 'poly']),\n",
    "            'C': trial.suggest_float('svm_C', 0.1, 10.0)\n",
    "        }\n",
    "        model = SVR(**params)\n",
    "\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10)\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "\n",
    "    else:  # Linear Regression\n",
    "        model = LinearRegression()\n",
    "\n",
    "    # Fit the model and make predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100  # MAPE as percentage\n",
    "\n",
    "    return r2  # Return R-squared value for optimization\n",
    "\n",
    "# Create the Optuna study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nModel Performance Metrics-----\")\n",
    "\n",
    "# Evaluate and print results for each model\n",
    "models = ['Random Forest', 'SVM', 'XGBoost', 'Linear Regression', 'Gradient Boosting']\n",
    "for model_name in models:\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "\n",
    "    # Initialize and fit the model with the best parameters\n",
    "    if model_name == 'Random Forest':\n",
    "        best_model = RandomForestRegressor(n_estimators=study.best_params.get('n_estimators', 100),\n",
    "                                            max_depth=study.best_params.get('max_depth', 10))\n",
    "    elif model_name == 'XGBoost':\n",
    "        best_model = XGBRegressor(\n",
    "            learning_rate=study.best_params.get('learning_rate', 0.1),\n",
    "            max_depth=study.best_params.get('max_depth', 3),\n",
    "            n_estimators=study.best_params.get('n_estimators', 100),\n",
    "            min_child_weight=study.best_params.get('min_child_weight', 1),\n",
    "            subsample=study.best_params.get('subsample', 1.0),\n",
    "            gamma=study.best_params.get('gamma', 0)\n",
    "        )\n",
    "    elif model_name == 'SVM':\n",
    "        best_model = SVR(\n",
    "            kernel=study.best_params.get('svm_kernel', 'linear'),\n",
    "            C=study.best_params.get('svm_C', 1.0)\n",
    "        )\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        best_model = GradientBoostingRegressor(\n",
    "            learning_rate=study.best_params.get('learning_rate', 0.1),\n",
    "            n_estimators=study.best_params.get('n_estimators', 100),\n",
    "            max_depth=study.best_params.get('max_depth', 3),\n",
    "            min_samples_split=study.best_params.get('min_samples_split', 2)\n",
    "        )\n",
    "    else:  # Linear Regression\n",
    "        best_model = LinearRegression()\n",
    "\n",
    "    # Fit the model and make predictions\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100  # MAPE as percentage\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"{model_name} MSE: {mse:.4f}\")\n",
    "    print(f\"{model_name} RMSE: {rmse:.4f}\")\n",
    "    print(f\"{model_name} R-squared: {r2:.4f}\")\n",
    "    print(f\"{model_name} MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTDOA_jt2Qvh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
